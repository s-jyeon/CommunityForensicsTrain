{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference & Submission using Trained Model (.pt file)\n",
    "This notebook loads a trained model from .pt file and performs inference on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hugging Face model imports\n",
    "import models\n",
    "import dataprocessor_hf as dphf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings\n",
    "BASE_MODEL_NAME = 'OwensLab/commfor-model-384'  # 기본 모델 구조\n",
    "PROCESSOR_NAME = 'OwensLab/commfor-data-preprocessor'\n",
    "INPUT_SIZE = 384\n",
    "\n",
    "# Trained model path\n",
    "TRAINED_MODEL_PATH = './trained_model/20260130-032827/commfor_train_best.pt'  # 학습된 모델 경로 (수정 필요)\n",
    "\n",
    "# Data paths\n",
    "TEST_DIR = Path(\"./test_images\")  # test 데이터 경로\n",
    "\n",
    "# Submission\n",
    "OUTPUT_DIR = Path(\"./output\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_CSV = OUTPUT_DIR / \"trained_model_submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "IMAGE_EXTS = {\".jpg\", \".jpeg\", \".png\", \".jfif\"}\n",
    "VIDEO_EXTS = {\".mp4\", \".mov\"}\n",
    "\n",
    "TARGET_SIZE = (384, 384)\n",
    "NUM_FRAMES = 10  # 비디오 샘플링 프레임 수\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_frame_indices(total_frames: int, num_frames: int) -> np.ndarray:\n",
    "    \"\"\"비디오 프레임을 균등하게 샘플링\"\"\"\n",
    "    if total_frames <= 0:\n",
    "        return np.array([], dtype=int)\n",
    "    if total_frames <= num_frames:\n",
    "        return np.arange(total_frames, dtype=int)\n",
    "    return np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "\n",
    "def get_full_frame_padded(pil_img: Image.Image, target_size=(384, 384)) -> Image.Image:\n",
    "    \"\"\"전체 이미지를 비율 유지하며 정사각형 패딩 처리\"\"\"\n",
    "    img = pil_img.convert(\"RGB\")\n",
    "    img.thumbnail(target_size, Image.BICUBIC)\n",
    "    new_img = Image.new(\"RGB\", target_size, (0, 0, 0))\n",
    "    new_img.paste(img, ((target_size[0] - img.size[0]) // 2,\n",
    "                        (target_size[1] - img.size[1]) // 2))\n",
    "    return new_img\n",
    "\n",
    "def read_rgb_frames(file_path: Path, num_frames: int = NUM_FRAMES) -> List[np.ndarray]:\n",
    "    \"\"\"이미지 또는 비디오에서 RGB 프레임 추출\"\"\"\n",
    "    ext = file_path.suffix.lower()\n",
    "    \n",
    "    # 이미지 파일\n",
    "    if ext in IMAGE_EXTS:\n",
    "        try:\n",
    "            img = Image.open(file_path).convert(\"RGB\")\n",
    "            return [np.array(img)]\n",
    "        except Exception:\n",
    "            return []\n",
    "    \n",
    "    # 비디오 파일\n",
    "    if ext in VIDEO_EXTS:\n",
    "        cap = cv2.VideoCapture(str(file_path))\n",
    "        total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        if total <= 0:\n",
    "            cap.release()\n",
    "            return []\n",
    "        \n",
    "        frame_indices = uniform_frame_indices(total, num_frames)\n",
    "        frames = []\n",
    "        \n",
    "        for idx in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, int(idx))\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "            frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        cap.release()\n",
    "        return frames\n",
    "    \n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessOutput:\n",
    "    def __init__(\n",
    "        self,\n",
    "        filename: str,\n",
    "        imgs: List[Image.Image],\n",
    "        error: Optional[str] = None\n",
    "    ):\n",
    "        self.filename = filename\n",
    "        self.imgs = imgs\n",
    "        self.error = error\n",
    "\n",
    "def preprocess_one(file_path: Path, num_frames: int = NUM_FRAMES) -> PreprocessOutput:\n",
    "    \"\"\"\n",
    "    파일 하나에 대한 전처리 수행\n",
    "    \n",
    "    Args:\n",
    "        file_path: 처리할 파일 경로\n",
    "        num_frames: 비디오에서 추출할 프레임 수\n",
    "    \n",
    "    Returns:\n",
    "        PreprocessOutput 객체\n",
    "    \"\"\"\n",
    "    try:\n",
    "        frames = read_rgb_frames(file_path, num_frames=num_frames)\n",
    "              \n",
    "        imgs: List[Image.Image] = []\n",
    "        \n",
    "        for rgb in frames:     \n",
    "            imgs.append(get_full_frame_padded(Image.fromarray(rgb), TARGET_SIZE))\n",
    "        \n",
    "        return PreprocessOutput(file_path.name, imgs, None)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return PreprocessOutput(file_path.name, [], str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data processor and model...\n",
      "Data processor loaded: OwensLab/commfor-data-preprocessor\n",
      "Base model structure loaded: OwensLab/commfor-model-384\n",
      "Loading trained weights from: ./trained_model/20260130-032827/commfor_train_best.pt\n",
      "Trained weights loaded successfully!\n",
      "Model is on device: cuda:0\n",
      "Model ready for inference!\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data processor and model...\")\n",
    "\n",
    "# Load data processor from Hugging Face\n",
    "data_processor = dphf.CommForImageProcessor.from_pretrained(\n",
    "    PROCESSOR_NAME, \n",
    "    size=INPUT_SIZE\n",
    ")\n",
    "print(f\"Data processor loaded: {PROCESSOR_NAME}\")\n",
    "\n",
    "# Load base model structure from Hugging Face\n",
    "model = models.ViTClassifier.from_pretrained(BASE_MODEL_NAME)\n",
    "print(f\"Base model structure loaded: {BASE_MODEL_NAME}\")\n",
    "\n",
    "# Load trained weights from .pt file\n",
    "print(f\"Loading trained weights from: {TRAINED_MODEL_PATH}\")\n",
    "state_dict = torch.load(\n",
    "    TRAINED_MODEL_PATH,\n",
    "    map_location=DEVICE\n",
    ")\n",
    "\n",
    "# Load state dict into model\n",
    "model.load_state_dict(state_dict)\n",
    "print(\"Trained weights loaded successfully!\")\n",
    "\n",
    "# Move model to device and set to eval mode\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "print(\"Model ready for inference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def infer_fake_probs(imgs):\n",
    "    probs = []\n",
    "    for img in imgs:\n",
    "        # PIL.Image -> torch.Tensor\n",
    "        if isinstance(img, Image.Image):\n",
    "            img = to_tensor(img)\n",
    "\n",
    "        # Tensor shape 확인: (C,H,W) -> (1,C,H,W)\n",
    "        if img.ndim == 3:\n",
    "            img = img.unsqueeze(0)\n",
    "\n",
    "        img = img.to(device)  # 전역 device 사용\n",
    "        with torch.no_grad():\n",
    "            logit = model(img)\n",
    "            prob = torch.sigmoid(logit).item()\n",
    "            probs.append(prob)\n",
    "    return probs\n",
    "\n",
    "# 변환 정의 (이미지 -> tensor, 0~1 정규화)\n",
    "to_tensor = T.Compose([\n",
    "    T.Resize((384, 384)),  # 모델 input size와 맞추기\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "def infer_fake_probs(imgs):\n",
    "    probs = []\n",
    "    for img in imgs:\n",
    "        # PIL.Image -> torch.Tensor\n",
    "        if isinstance(img, Image.Image):\n",
    "            img = to_tensor(img)\n",
    "\n",
    "        # Tensor shape 확인\n",
    "        # img: (C,H,W) -> (1,C,H,W)\n",
    "        if img.ndim == 3:\n",
    "            img = img.unsqueeze(0)\n",
    "\n",
    "        img = img.to(device)  # 모델 device\n",
    "        with torch.no_grad():\n",
    "            logit = model(img)\n",
    "            prob = torch.sigmoid(logit).item()\n",
    "            probs.append(prob)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Inference on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data length: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 10/10 [00:05<00:00,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference completed. Processed: 10 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get test files\n",
    "files = sorted([p for p in TEST_DIR.iterdir() if p.is_file()])\n",
    "print(f\"Test data length: {len(files)}\")\n",
    "\n",
    "results: Dict[str, float] = {}\n",
    "\n",
    "# 전처리 및 추론\n",
    "for file_path in tqdm(files, desc=\"Processing\"):\n",
    "    out = preprocess_one(file_path)\n",
    "    \n",
    "    # 1. 에러 로깅\n",
    "    if out.error:\n",
    "        print(f\"[WARN] {out.filename}: {out.error}\")\n",
    "        results[out.filename] = 0.0\n",
    "    \n",
    "    # 2. 정상 추론\n",
    "    elif out.imgs:\n",
    "        probs = infer_fake_probs(out.imgs)\n",
    "        results[out.filename] = float(np.mean(probs)) if probs else 0.0\n",
    "    \n",
    "    # 3. 둘 다 없으면 0.0 (real)\n",
    "    else:\n",
    "        results[out.filename] = 0.0\n",
    "\n",
    "print(f\"Inference completed. Processed: {len(results)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'00000274.png': 0.6781665682792664, '00000420.png': 0.9799940586090088, '00000845.png': 0.2748264968395233, '00000916.png': 0.9986201524734497, '00000989.png': 0.9167060256004333, 'faceswap1.png': 0.24647334218025208, 'faceswap2.png': 0.05445140227675438, 'faceswap3.png': 0.9823058247566223, 'faceswap4.png': 0.9744099974632263, 'realimage_genvideo_kling_20260129_Image_to_Video_A_very_sho_3811_0.mp4': 0.9953669846057892}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample submission file\n",
    "# Note: Update the path to your actual sample_submission.csv file\n",
    "SAMPLE_SUBMISSION_PATH = './sample_submission.csv'\n",
    "\n",
    "submission = pd.read_csv(SAMPLE_SUBMISSION_PATH)\n",
    "submission['prob'] = submission['filename'].map(results).fillna(0.0)\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nSubmission preview:\")\n",
    "print(submission.head(10))\n",
    "\n",
    "print(\"\\nSubmission statistics:\")\n",
    "print(f\"Total files: {len(submission)}\")\n",
    "print(f\"Mean probability: {submission['prob'].mean():.4f}\")\n",
    "print(f\"Min probability: {submission['prob'].min():.4f}\")\n",
    "print(f\"Max probability: {submission['prob'].max():.4f}\")\n",
    "\n",
    "# Save submission\n",
    "submission.to_csv(OUT_CSV, encoding='utf-8-sig', index=False)\n",
    "print(f\"\\nSaved submission to: {OUT_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on Sample Images (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is optional - test on a few sample images to verify the model works\n",
    "# You can skip this cell if you don't have test images\n",
    "\n",
    "# Example test images (update paths as needed)\n",
    "test_images_paths = [\n",
    "    # Add your test image paths here\n",
    "    # \"test_images/00000274.png\",\n",
    "    # \"test_images/00000420.png\",\n",
    "]\n",
    "\n",
    "if test_images_paths and all(Path(p).exists() for p in test_images_paths):\n",
    "    test_imgs = [Image.open(p).convert('RGB') for p in test_images_paths]\n",
    "    \n",
    "    # Run inference\n",
    "    probs = infer_fake_probs(test_imgs)\n",
    "    \n",
    "    print(\"\\nTest image results:\")\n",
    "    for path, prob in zip(test_images_paths, probs):\n",
    "        print(f\"{Path(path).name}: {prob:.4f}\")\n",
    "else:\n",
    "    print(\"No test images found or paths not configured. Skipping sample test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative: Load from WandB Artifact (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you saved your model as a WandB artifact (like in model_training.ipynb),\n",
    "# you can load it using this method instead\n",
    "\n",
    "'''\n",
    "import wandb\n",
    "\n",
    "# Initialize WandB\n",
    "run = wandb.init(project=\"your-project-name\", job_type=\"inference\")\n",
    "\n",
    "# Download artifact\n",
    "artifact = run.use_artifact(\n",
    "    \"your-entity/your-project/model-name:version\",\n",
    "    type=\"model\"\n",
    ")\n",
    "\n",
    "artifact_dir = artifact.download()\n",
    "print(f\"Artifact downloaded to: {artifact_dir}\")\n",
    "\n",
    "# Load model from artifact\n",
    "model = models.ViTClassifier.from_pretrained(BASE_MODEL_NAME)\n",
    "state_dict = torch.load(\n",
    "    f\"{artifact_dir}/your_model.pt\",\n",
    "    map_location=DEVICE\n",
    ")\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded from WandB artifact!\")\n",
    "'''\n",
    "\n",
    "print(\"WandB artifact loading code is commented out. Uncomment and configure if needed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
