{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Inference & Submission using Hugging Face Model\n",
    "This notebook uses the OwensLab/commfor-model-384 from Hugging Face for deepfake detection inference and submission generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hugging Face model imports\n",
    "import models\n",
    "import dataprocessor_hf as dphf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings\n",
    "MODEL_NAME = 'OwensLab/commfor-model-384'\n",
    "PROCESSOR_NAME = 'OwensLab/commfor-data-preprocessor'\n",
    "INPUT_SIZE = 384\n",
    "\n",
    "# Data paths\n",
    "TEST_DIR = Path(\"./test_images\")  # test 데이터 경로\n",
    "\n",
    "# Submission\n",
    "OUTPUT_DIR = Path(\"./output\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SAFE_MODEL_NAME = MODEL_NAME.replace(\"/\", \"_\")\n",
    "OUT_CSV = OUTPUT_DIR / f\"{SAFE_MODEL_NAME}_submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "IMAGE_EXTS = {\".jpg\", \".jpeg\", \".png\", \".jfif\"}\n",
    "VIDEO_EXTS = {\".mp4\", \".mov\"}\n",
    "\n",
    "TARGET_SIZE = (384, 384)\n",
    "NUM_FRAMES = 10  # 비디오 샘플링 프레임 수\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_frame_indices(total_frames: int, num_frames: int) -> np.ndarray:\n",
    "    \"\"\"비디오 프레임을 균등하게 샘플링\"\"\"\n",
    "    if total_frames <= 0:\n",
    "        return np.array([], dtype=int)\n",
    "    if total_frames <= num_frames:\n",
    "        return np.arange(total_frames, dtype=int)\n",
    "    return np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "\n",
    "def get_full_frame_padded(pil_img: Image.Image, target_size=(384, 384)) -> Image.Image:\n",
    "    \"\"\"전체 이미지를 비율 유지하며 정사각형 패딩 처리\"\"\"\n",
    "    img = pil_img.convert(\"RGB\")\n",
    "    img.thumbnail(target_size, Image.BICUBIC)\n",
    "    new_img = Image.new(\"RGB\", target_size, (0, 0, 0))\n",
    "    new_img.paste(img, ((target_size[0] - img.size[0]) // 2,\n",
    "                        (target_size[1] - img.size[1]) // 2))\n",
    "    return new_img\n",
    "\n",
    "def read_rgb_frames(file_path: Path, num_frames: int = NUM_FRAMES) -> List[np.ndarray]:\n",
    "    \"\"\"이미지 또는 비디오에서 RGB 프레임 추출\"\"\"\n",
    "    ext = file_path.suffix.lower()\n",
    "    \n",
    "    # 이미지 파일\n",
    "    if ext in IMAGE_EXTS:\n",
    "        try:\n",
    "            img = Image.open(file_path).convert(\"RGB\")\n",
    "            return [np.array(img)]\n",
    "        except Exception:\n",
    "            return []\n",
    "    \n",
    "    # 비디오 파일\n",
    "    if ext in VIDEO_EXTS:\n",
    "        cap = cv2.VideoCapture(str(file_path))\n",
    "        total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        if total <= 0:\n",
    "            cap.release()\n",
    "            return []\n",
    "        \n",
    "        frame_indices = uniform_frame_indices(total, num_frames)\n",
    "        frames = []\n",
    "        \n",
    "        for idx in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, int(idx))\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "            frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        cap.release()\n",
    "        return frames\n",
    "    \n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessOutput:\n",
    "    def __init__(\n",
    "        self,\n",
    "        filename: str,\n",
    "        imgs: List[Image.Image],\n",
    "        error: Optional[str] = None\n",
    "    ):\n",
    "        self.filename = filename\n",
    "        self.imgs = imgs\n",
    "        self.error = error\n",
    "\n",
    "def preprocess_one(file_path: Path, num_frames: int = NUM_FRAMES) -> PreprocessOutput:\n",
    "    \"\"\"\n",
    "    파일 하나에 대한 전처리 수행\n",
    "    \n",
    "    Args:\n",
    "        file_path: 처리할 파일 경로\n",
    "        num_frames: 비디오에서 추출할 프레임 수\n",
    "    \n",
    "    Returns:\n",
    "        PreprocessOutput 객체\n",
    "    \"\"\"\n",
    "    try:\n",
    "        frames = read_rgb_frames(file_path, num_frames=num_frames)\n",
    "              \n",
    "        imgs: List[Image.Image] = []\n",
    "        \n",
    "        for rgb in frames:     \n",
    "            imgs.append(get_full_frame_padded(Image.fromarray(rgb), TARGET_SIZE))\n",
    "        \n",
    "        return PreprocessOutput(file_path.name, imgs, None)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return PreprocessOutput(file_path.name, [], str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data processor and model...\n",
      "Model loaded: OwensLab/commfor-model-384\n",
      "Data processor loaded: OwensLab/commfor-data-preprocessor\n",
      "Model is on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data processor and model...\")\n",
    "\n",
    "# Load data processor from Hugging Face\n",
    "data_processor = dphf.CommForImageProcessor.from_pretrained(\n",
    "    PROCESSOR_NAME, \n",
    "    size=INPUT_SIZE\n",
    ")\n",
    "\n",
    "# Load model from Hugging Face\n",
    "model = models.ViTClassifier.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded: {MODEL_NAME}\")\n",
    "print(f\"Data processor loaded: {PROCESSOR_NAME}\")\n",
    "print(f\"Model is on device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def infer_fake_probs(imgs):\n",
    "    probs = []\n",
    "    for img in imgs:\n",
    "        # PIL.Image -> torch.Tensor\n",
    "        if isinstance(img, Image.Image):\n",
    "            img = to_tensor(img)\n",
    "\n",
    "        # Tensor shape 확인: (C,H,W) -> (1,C,H,W)\n",
    "        if img.ndim == 3:\n",
    "            img = img.unsqueeze(0)\n",
    "\n",
    "        img = img.to(device)  # 전역 device 사용\n",
    "        with torch.no_grad():\n",
    "            logit = model(img)\n",
    "            prob = torch.sigmoid(logit).item()\n",
    "            probs.append(prob)\n",
    "    return probs\n",
    "\n",
    "# 변환 정의 (이미지 -> tensor, 0~1 정규화)\n",
    "to_tensor = T.Compose([\n",
    "    T.Resize((384, 384)),  # 모델 input size와 맞추기\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "def infer_fake_probs(imgs):\n",
    "    probs = []\n",
    "    for img in imgs:\n",
    "        # PIL.Image -> torch.Tensor\n",
    "        if isinstance(img, Image.Image):\n",
    "            img = to_tensor(img)\n",
    "\n",
    "        # Tensor shape 확인\n",
    "        # img: (C,H,W) -> (1,C,H,W)\n",
    "        if img.ndim == 3:\n",
    "            img = img.unsqueeze(0)\n",
    "\n",
    "        img = img.to(device)  # 모델 device\n",
    "        with torch.no_grad():\n",
    "            logit = model(img)\n",
    "            prob = torch.sigmoid(logit).item()\n",
    "            probs.append(prob)\n",
    "    return probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Inference on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data length: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 10/10 [00:03<00:00,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference completed. Processed: 10 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get test files\n",
    "files = sorted([p for p in TEST_DIR.iterdir() if p.is_file()])\n",
    "print(f\"Test data length: {len(files)}\")\n",
    "\n",
    "results: Dict[str, float] = {}\n",
    "\n",
    "# 전처리 및 추론\n",
    "for file_path in tqdm(files, desc=\"Processing\"):\n",
    "    out = preprocess_one(file_path)\n",
    "    \n",
    "    # 1. 에러 로깅\n",
    "    if out.error:\n",
    "        print(f\"[WARN] {out.filename}: {out.error}\")\n",
    "        results[out.filename] = 0.0\n",
    "    \n",
    "    # 2. 정상 추론\n",
    "    elif out.imgs:\n",
    "        probs = infer_fake_probs(out.imgs)\n",
    "        results[out.filename] = float(np.mean(probs)) if probs else 0.0\n",
    "    \n",
    "    # 3. 둘 다 없으면 0.0 (real)\n",
    "    else:\n",
    "        results[out.filename] = 0.0\n",
    "\n",
    "print(f\"Inference completed. Processed: {len(results)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'00000274.png': 0.9872192144393921, '00000420.png': 0.9979440569877625, '00000845.png': 0.000356019358150661, '00000916.png': 0.9264843463897705, '00000989.png': 0.8770726919174194, 'faceswap1.png': 0.36961260437965393, 'faceswap2.png': 0.8823122978210449, 'faceswap3.png': 0.10990641266107559, 'faceswap4.png': 0.17557691037654877, 'realimage_genvideo_kling_20260129_Image_to_Video_A_very_sho_3811_0.mp4': 0.003448340226896107}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submission preview:\n",
      "       filename  prob\n",
      "0  TEST_000.mp4   0.0\n",
      "1  TEST_001.jpg   0.0\n",
      "2  TEST_002.mp4   0.0\n",
      "3  TEST_003.mp4   0.0\n",
      "4  TEST_004.jpg   0.0\n",
      "5  TEST_005.mp4   0.0\n",
      "6  TEST_006.mp4   0.0\n",
      "7  TEST_007.jpg   0.0\n",
      "8  TEST_008.jpg   0.0\n",
      "9  TEST_009.png   0.0\n",
      "\n",
      "Submission statistics:\n",
      "Total files: 500\n",
      "Mean probability: 0.0000\n",
      "Min probability: 0.0000\n",
      "Max probability: 0.0000\n",
      "\n",
      "Saved submission to: output/OwensLab_commfor-model-384_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Load sample submission file\n",
    "# Note: Update the path to your actual sample_submission.csv file\n",
    "SAMPLE_SUBMISSION_PATH = './sample_submission.csv'\n",
    "\n",
    "submission = pd.read_csv(SAMPLE_SUBMISSION_PATH)\n",
    "submission['prob'] = submission['filename'].map(results).fillna(0.0)\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nSubmission preview:\")\n",
    "print(submission.head(10))\n",
    "\n",
    "print(\"\\nSubmission statistics:\")\n",
    "print(f\"Total files: {len(submission)}\")\n",
    "print(f\"Mean probability: {submission['prob'].mean():.4f}\")\n",
    "print(f\"Min probability: {submission['prob'].min():.4f}\")\n",
    "print(f\"Max probability: {submission['prob'].max():.4f}\")\n",
    "\n",
    "# Save submission\n",
    "submission.to_csv(OUT_CSV, encoding='utf-8-sig', index=False)\n",
    "print(f\"\\nSaved submission to: {OUT_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on Sample Images (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is optional - test on a few sample images to verify the model works\n",
    "# You can skip this cell if you don't have test images\n",
    "\n",
    "# Example test images (update paths as needed)\n",
    "test_images_paths = [\n",
    "    # Add your test image paths here\n",
    "    # \"test_images/00000274.png\",\n",
    "    # \"test_images/00000420.png\",\n",
    "]\n",
    "\n",
    "if test_images_paths and all(Path(p).exists() for p in test_images_paths):\n",
    "    test_imgs = [Image.open(p).convert('RGB') for p in test_images_paths]\n",
    "    \n",
    "    # Run inference\n",
    "    probs = infer_fake_probs(test_imgs)\n",
    "    \n",
    "    print(\"\\nTest image results:\")\n",
    "    for path, prob in zip(test_images_paths, probs):\n",
    "        print(f\"{Path(path).name}: {prob:.4f}\")\n",
    "else:\n",
    "    print(\"No test images found or paths not configured. Skipping sample test.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
